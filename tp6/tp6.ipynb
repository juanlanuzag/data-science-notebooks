{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jlanuza/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jlanuza/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jlanuza/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jlanuza/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk as nk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import untangle\n",
    "import seaborn as sns\n",
    "import operator\n",
    "\n",
    "nk.download('punkt')\n",
    "nk.download('stopwords')\n",
    "nk.download('wordnet')\n",
    "nk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asociación de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Levantar el corpus AP, separando cada noticia como un elemento distinto en un diccionario (DOCNO :TEXT).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_ap = {}\n",
    "obj = untangle.parse('ap/ap.txt')\n",
    "for i in range(len(obj.root.DOC)):\n",
    "    documento = obj.root.DOC[i]\n",
    "    corpus_ap[documento.DOCNO.cdata.strip()] = documento.TEXT.cdata.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Calcular el tamaño del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for item in corpus_ap.items():\n",
    "    tokens = nk.sent_tokenize(item[1].lower())\n",
    "    for sent in tokens:\n",
    "        sentences.append(sent)\n",
    "len(sentences)\n",
    "\n",
    "def clean_chars(word):\n",
    "    word= word.replace(',',' ')\n",
    "    word= word.replace('.',' ')\n",
    "    word= word.replace(';',' ')\n",
    "    return word.strip()\n",
    "    \n",
    "words_by_sent = [list(map(clean_chars, s.split(' '))) for s in sentences]\n",
    "# words_by_sent2 = []\n",
    "# print words_by_sent[:2]\n",
    "for sent in words_by_sent:\n",
    "    for word in sent:\n",
    "        if word is not '':\n",
    "            words_by_sent2.append(word)\n",
    "word_semantic = nk.tag.pos_tag_sents(words_by_sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberto roberto\n",
      "bermudez, bermudez,\n",
      "32, 32,\n",
      "a a\n",
      "painter, painter,\n",
      "said say\n",
      "his his\n",
      "opposition opposition\n",
      "to to\n",
      "the the\n",
      "government government\n",
      "has have\n",
      "meant mean\n",
      "he he\n",
      "can't can't\n",
      "exhibit exhibit\n",
      "his his\n",
      "paintings. paintings.\n"
     ]
    }
   ],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return nk.corpus.wordnet.NOUN\n",
    "\n",
    "    \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for word_tuple in word_semantic[40]:\n",
    "    print word_tuple[0], lemmatizer.lemmatize(word_tuple[0], get_wordnet_pos(word_tuple[1]))\n",
    "\n",
    "# wnl = WordNetLemmatizer()\n",
    "# [(i, wnl.lemmatize(i,j[0].lower())) \n",
    "#  if j[0].lower() in ['a','n','v'] else (i, wnl.lemmatize(i))\n",
    "#  for i,j in word_semantic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopw = set(nk.corpus.stopwords.words(\"english\"))\n",
    "with open('word_count.pickle', 'rb') as handle:\n",
    "    word_count = pickle.load(handle) \n",
    "# Aca limpiamos las stopwords\n",
    "word_count = [(x, y) for (x,y) in sorted(word_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "                                                 if x not in stopw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Para las 500 palabras con m´as apariciones, calcular el par m´as asociado seg´un la medida presentada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informaci´on L´exica[2]\n",
    "Bajar de Project Gutenberg el libro de Darwin ON THE ORIGIN OF SPECIES."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Procesar el texto, tokenizando eliminando signos de puntuaci´on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Siguiendo el art´ıculo de la secci´on, calcular la autocorrelaci´on para estimar la distribuci´on de la palabra a lo largo del texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Armar una funci´on que reciba una lista de tokens, una lista de palabras y un tama˜no de ventana y devuelva una lista de probabilidades de encontrar la palabra en cada ventana para cada palabra pasada por par´ametro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Calcular la entrop´ıa de la distribuci´on de palabras seleccionadas para distintos tama˜nos de ventana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Generar una versi´on randomizada del texto, y medir la entrop´ıa de las palabras randomizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Distinguir las palabras del texto en art´ıculos, sustantivos y adjetivos usando un POS-tagger. Verificar si las medidas separan a estos grupos de palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings, distancia sem´antica y WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Utilizando el test WordSim3531, comparar el rendimiento entre LSA[3] y Word2Vec2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Comparar los distintos word embeddings con las medidas definidas en WordNet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
